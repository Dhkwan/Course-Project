---
title: "Course Project"
author: "Damon Kwan"
date: "2024-02-05"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(ggplot2)
library(tidyverse)
library(knitr)
library(ROCR)
library(dplyr)
library(class)
library(pROC)
library(MASS)
```

# Neural Activity in Mice

------------------------------------------------------------------------

## Abstract

In this project, I will build a predictive model. I will first analyze the neural activity of multiple mice across different sessions and trials. I will analyze how different variables impact the feedback type across these sessions. In particular, I will test how the spikes and the stimuli of the neural activity affect the type of feedback and the overall success rate. Across the different sessions and trials, I will find patterns using plots. After finding such patterns, I will use this to build my predictive model. I will test out different types of combinations of data to see which will give me the best predictive model. I would use this model to predict a test set afterwards.

## Section 1 Introduction

I will use a subset of data collected by Steinmetz et al. (2019) to build a predictive model that investigates the how spikes and stimuli affect the feedback type. In this study, Neuropixel probes were used to record from around 30000 neurons in 42 brain regions where mice performed a visual task. In my subset, 10 mice were experimented on across 18 different sessions, with hundreds of trial each session. The visual task in this particular subset involved showing a mouse visual stimuli on two screens on the left and the right side. The visual stimuli had contrast levels {0, .25, .5, 1} (0 being no stimulus). In response to the stimuli, the mice were suppose to interact with the wheel. Based on their interaction, a reward was given. The neural activity were shown using spikes and have time stamps of neuron firing. I am looking on the spike trains of neurons 0.4 seconds post-onset.

## Section 2 Exploratory Analysis

Let's first create a table from each session detailing different types of variables used in the study.

```{r, cache=TRUE}
session <- list()
data_table <- tibble(
  Session_number = rep(0, 18),
  Name = rep("Name", 18), 
  Neuron_number = rep(0, 18),
  Trial_number = rep(0, 18),
  Brain_area = rep(0,18),
  Success_Rate = rep(0, 18)
)
session <- list()
for (i in 1:18) {
  session[[i]] <- readRDS(paste('./Data/session', i, '.rds', sep=''))
  data_table[i,1] <- i
  data_table[i,2] <- session[[i]]$mouse_name
  data_table[i,3] <- dim(session[[i]]$spks[[1]])[1]
  data_table[i,4] <- length(session[[i]]$spks)
  data_table[i,5] <- length(unique(session[[i]]$brain_area))
  data_table[i,6] <- (sum(session[[i]]$feedback_type == 
                            1))/(length(session[[i]]$feedback_type))
}
colnames(data_table) <- c("Session Number", "Mouse Name", "Neuron Number", "Trial Number", 
                          "Brain Area", "Success Rate")
kable(data_table, format = "html", table.attr = "class='table table-striped'",digits=2) 
minSuccess <- min(data_table$`Success Rate`)
maxSuccess <- max(data_table$`Success Rate`)
```

We can see that across all 18 sessions, the success rate for each session is between `r minSuccess` and `r maxSuccess`. This range is quite big, so let's check if there are any explanations for this range in success rate.

First, Let's see if there is any correlation in the name of the mouse and the success rate.

```{r cache=TRUE}
ggplot(data_table, aes(x = `Mouse Name`, y = `Success Rate`)) +
  geom_point() +
  labs(title = "Average Success Rate by Mouse",
       x = "Name",
       y = "Average Success Rate")
```

The graph shows the distribution of average success rate for each mouse. From the graph, we can see that the mouse named Cori has a significantly lower success rate than the mouse named Lederberg. The other mice seem to have a spread out success rate. The reason for the change in success rate may be because of the intelligence of certain mice. The mouse named Lederberg could be smarter than the mouse named Cori, so we see a higher success rate in Lederberg.

Let's now see if the number of neurons activated has any influence on success rate.

```{r cache=TRUE}
ggplot(data_table, aes(x = `Neuron Number`, y = `Success Rate`)) +
  geom_point() + geom_smooth(method = 'loess', formula = 'y ~ x') +
  labs(title = "Average Success Rate by Neuron Activation",
       x = "Neuron Activation",
       y = "Average Success Rate") +
  coord_cartesian(xlim = c(600, 1800))

```

This plot graphs the number of Neuron Activation with success rate. There seems to be a small peak near 1000 neurons activated and sharply declines after. This could be because more neuron activation indicates higher intelligence early on, yet to much activation makes the mice tired, and preform poorly. Although this could be a plausible reason, the pattern is not clear enough and more testing is necessary.

Let's check if the number of trials has any influence.

```{r cache=TRUE}
ggplot(data_table, aes(x = `Trial Number`, y = `Success Rate`)) +
  geom_point() +  geom_smooth(method = 'loess', formula = 'y ~ x') +
  labs(title = "Average Success Rate by Trial Number",
       x = "Trial Number",
       y = "Average Success Rate") +
  coord_cartesian(xlim = c(100, 450))

```

This plot graphs trial number by success rate. There appears to be a positive peak near the 175th trial mark. Additionally there's a negative peak around the 250th trial mark. After the negative peak there is an increase, then a decrease. This graph does not seem to have a clear pattern, so the number of trials does not have a clear impact on success rate.

We can also check if there's a relation with the number of brain area activated.

```{r cache=TRUE}
ggplot(data_table, aes(x = `Brain Area`, y = `Success Rate`)) +
  geom_point() + geom_smooth(method = 'loess', formula = 'y ~ x') +
  labs(title = "Average Success Rate by Unique Brain Area",
       x = "Unique Brain Area",
       y = "Average Success Rate") +
  coord_cartesian(xlim = c(5,15))

```

The graph above plots average success rate against number of unique brain areas. Again, there is no clear pattern with brain area and success rate, so the number of unique brain area activated does not have a clear impact on success rate.

Let's check whether or not the mice have some sort of biased to turning the wheel right, left, or doing nothing at all. To do so, we compare the outcome of the mouse and the desired outcome.

```{r cache=TRUE}
master_vector <- vector("list", length = 18)
for (k in 1:18) {
  contrast_list <- vector("character", length = length(session[[k]]$feedback_type))
  for (i in 1:length(session[[k]]$contrast_left)){
    if (session[[k]]$contrast_left[i] > session[[k]]$contrast_right[i]) {
      contrast_list[i] <- ("Left")
    } else if (session[[k]]$contrast_left[i] < session[[k]]$contrast_right[i])
      {
      contrast_list[i] <- ("Right")
    } else if (session[[k]]$contrast_left[i] + session[[k]]$contrast_right[i] == 0){
      contrast_list[i] <- ("zero")
    }else if (session[[k]]$contrast_left[i] == session[[k]]$contrast_right[i]){
      contrast_list[i] <- ("equal")
    }else {
      contrast_list[i] <- ("error")
    }
  }
  master_vector[[k]] <- contrast_list
}

proportionTable <- tibble(
  Session_number = rep(0, 18),
  right = rep(0, 18),
  left = rep(0,18),
  nothing = rep(0, 18)
)

for (i in 1:18){
  tmp <- table(master_vector[[i]],session[[i]]$feedback_type)
  proportionTable[i,1] <- i
  proportionTable[i,2] <- sum(tmp[3,])/sum(tmp[-1,])
  proportionTable[i,3] <- sum(tmp[2,])/sum(tmp[-1,])
  proportionTable[i,4] <- sum(tmp[4,])/sum(tmp[-1,])
}
colnames(proportionTable) <- c("Session Number", "Right", "Left", "Nothing") 
kable((proportionTable), format = "html", table.attr = "class='table table-striped'",digits=2) 
```

The table above shows the proportion of times the scientist wants the mice to turn right, left, and do nothing turns. Let's make a bar plot for easier visualization.

```{r cache=TRUE}
proportionTable.1 <- tibble(
  Session_number = rep(0, 18),
  right = rep(0, 18),
  left = rep(0,18),
  nothing = rep(0, 18)
)
for (i in 1:18){
  tmp <- table(master_vector[[i]],session[[i]]$feedback_type)
  proportionTable.1[i,1] <- i
  proportionTable.1[i,2] <- tmp[3,2]/sum(tmp[-1,-1])
  proportionTable.1[i,3] <- tmp[2,2]/sum(tmp[-1,-1])
  proportionTable.1[i,4] <- tmp[4,2]/sum(tmp[-1,-1])
}

averageProTable <- tibble(
  Right <- mean(proportionTable.1$right),
  Left <- mean(proportionTable.1$left),
  Nothing <- mean(proportionTable.1$nothing)
)

averageProTable2 <- tibble(
  Right <- mean(proportionTable$Right),
  Left <- mean(proportionTable$Left),
  Nothing <- mean(proportionTable$Nothing)
)
colnames(averageProTable) <- c("Right", "Left", "Nothing")
colnames(averageProTable2) <- c("Right", "Left", "Nothing")

combinedAverageProTable <- rbind(averageProTable, averageProTable2)

colnames(combinedAverageProTable) <- c("Right", "Left", "Nothing")

barplot(as.matrix(combinedAverageProTable), beside = TRUE, col = c("#e41a1c", "#377eb8"),
        main = "Right Vs Left Vs Nothing",
        xlab = "Action", ylab = "Proportions")

legend("topright", legend = c("Where the mice turned", "Desired Outcome"), fill = c("#e41a1c", "#377eb8"))
```

In the above graph, it plots the proportion of all mice across all sessions and trials. In red, it shows whether the mice would turn right, left, or do nothing. In blue, it details the scientists desired outcome. We can see that the scientist desire the mice to turn right or left rather than do nothing. This makes sense since the scientists are trying to measure a reaction; not a lack of one. In response to this, the mice chooses to preform a left or right turn more often than desired. This indicates importance in the right and left contrast variables. Regarding the desired amount of times the scientist wants the mice to turn right or left, they are about the same. However, the mice chooses to turn left more often. This indicates a slight bias towards turning left.

Since more research is needed, we will look at the spike data.

```{r cache=TRUE}
i.s=1 # indicator for this session

i.t=1

n.trial=length(session[[i.s]]$feedback_type)
n.area=length(unique(session[[i.s]]$brain_area ))
trial.summary =matrix(nrow=n.trial,ncol= n.area+1+2+1)

average_spike_area<-function(i.t,this_session){
  spk.trial = this_session$spks[[i.t]]
  area= this_session$brain_area
  spk.count=apply(spk.trial,1,sum)
  spk.average.tapply=tapply(spk.count, area, mean)
  return(spk.average.tapply)
}

for(i.t in 1:n.trial){
  trial.summary[i.t,]=c(average_spike_area(i.t,this_session = session[[i.s]]),
                          session[[i.s]]$feedback_type[i.t],
                        session[[i.s]]$contrast_left[i.t],
                        session[[i.s]]$contrast_right[i.s],
                        i.t)
}

colnames(trial.summary)=c(names(average_spike_area(i.t,this_session = session[[i.s]])), 'feedback', 'left contr.','right contr.','id' )

# Turning it into a data frame
trial.summary <- as_tibble(trial.summary)



area.col=rainbow(n=n.area,alpha=0.7)
# In base R, I usually initiate a blank plot before drawing anything on it
plot(x=1,y=0, col='white',xlim=c(0,n.trial),ylim=c(0.5,3), xlab="Trials",ylab="Average spike counts", main=paste("Spikes per area in Session", i.s))


for(i in 1:n.area){
  lines(y=trial.summary[[i]],x=trial.summary$id,col=area.col[i],lty=2,lwd=1)
  lines(smooth.spline(trial.summary$id, trial.summary[[i]]),col=area.col[i],lwd=3)
  }
legend("topright", 
  legend = colnames(trial.summary)[1:n.area], 
  col = area.col, 
  lty = 1, 
  cex = 0.8
)
```

The above graph details spikes in brain activity as the trials go on. It details different brain areas such as ACA, CA3, DG and so on. We see as trials go on, spikes in brain activity tend to decrease. This is important to note because we can see how each trial may cause exhaustion in mice. It is also worth noting that there is a slight increase in brain activity in CA3 until about trial number 40, then there is a decrease.

Now, let's look at the brain activity of session 17, the one with the highest success rate.

```{r cache = TRUE}
i.s= 17
i.t = 1

n.trial=length(session[[i.s]]$feedback_type)
n.area=length(unique(session[[i.s]]$brain_area ))
trial.summary =matrix(nrow=n.trial,ncol= n.area+1+2+1)

area.col=rainbow(n=n.area,alpha=0.7)
# In base R, I usually initiate a blank plot before drawing anything on it
plot(x=1,y=0, col='white',xlim=c(0,n.trial),ylim=c(0.5,3), xlab="Trials",ylab="Average spike counts", main=paste("Spikes per area in Session", i.s))

for(i.t in 1:n.trial){
  trial.summary[i.t,]=c(average_spike_area(i.t,this_session = session[[i.s]]),
                          session[[i.s]]$feedback_type[i.t],
                        session[[i.s]]$contrast_left[i.t],
                        session[[i.s]]$contrast_right[i.s],
                        i.t)
}

colnames(trial.summary)=c(names(average_spike_area(i.t,this_session = session[[i.s]])), 'feedback', 'left contr.','right contr.','id' )

# Turning it into a data frame
trial.summary <- as_tibble(trial.summary)


for(i in 1:n.area){
  lines(y=trial.summary[[i]],x=trial.summary$id,col=area.col[i],lty=2,lwd=1)
  lines(smooth.spline(trial.summary$id, trial.summary[[i]]),col=area.col[i],lwd=3)
}

legend("topright", 
  legend = colnames(trial.summary)[1:n.area], 
  col = area.col, 
  lty = 1, 
  cex = 0.8
)
```

Similarly to the first graph, the new graph details average spike count for each trial number. Although similar, this new graph shows a lot more fluctuation in spikes. This is especially true for LD, RT, and VPL brain activity. I can also see that there is no clear decline in brain activity like there was for the first session. I can still say mice get tired as trials go on. The mouse in session 17 did not see an overall decrease in spikes, and still received the highest success rate. This could indicated that the spike rate may be a good predictor for success rate.

## Section 3 Data Integration

From my exploratory analysis, I see that left contrast, right contrast, and average spike rate show interesting results when graphing. Thus, I will use these three variables to base my predictive model on.

```{r cache = TRUE}
avgSpks <- vector("list", length = 18)
for (i in 1:18) {
  spks.trial=session[[i]]$spks[[1]]
  total.spikes=apply(spks.trial,1,sum)
  avg.spikes=mean(total.spikes)
  avgSpks[i] <- avg.spikes
  #firing.rate <- apply(spks.trial,2,mean)
  #plot(firing.rate, type = "l")
}
```

I want to test out different combinations of each session and trial to see which would give me the most accurate predictive model. As a base line, I will test the average of all trials in each session.

```{r cache=TRUE}
data_integration <- tibble(
  Session_number = rep(0, 18),
  Name = rep("Name", 18), 
  Trial_number = rep(0, 18),
  Left_contrast = rep(0, 18),
  Right_contrast = rep(0, 18),
  AverageSpks = rep(0,18),
  SuccessRate = rep(0, 18)
)
data_integration[1] <- data_table[1]
data_integration[2] <- data_table[2]
data_integration[3] <- data_table[4]
data_integration[4] <- proportionTable[3]
data_integration[5] <- proportionTable[2]
data_integration[6] <- t(as.data.frame(avgSpks))
data_integration[7] <- data_table[6]

colnames(data_integration) <- c("Session Number", "Mouse Name", "Trial Number", "Left Contrast", "Right Contrast", "Average Spikes", "Success Rate")
kable(data_integration, format = "html", table.attr = "class='table table-striped'",digits=2) 
```

```{r cache=TRUE}
tableFunction <- function(sessionNumber) {
  averageSpikes <- vector("list", length = length(session[[sessionNumber]]$spks))
  
  for (i in 1:length(session[[sessionNumber]]$spks)) {
    spk.trial <- session[[sessionNumber]]$spks[[i]]
    total.spike <- apply(spk.trial, 1, sum)
    averageSpikes[[i]] <- mean(total.spike)
  }
  
  sessionTable <- tibble(
    Session_Number = rep(sessionNumber, times = length(session[[sessionNumber]]$spks)),
    Mouse_Name = rep(session[[sessionNumber]]$mouse_name, times = length(session[[sessionNumber]]$spks)),
    Trial_Number = 1:length(session[[sessionNumber]]$spks),
    Left_Contrast = session[[sessionNumber]]$contrast_left,
    Right_Contrast = session[[sessionNumber]]$contrast_right,
    Average_Spikes = unlist(averageSpikes),
    Feedback_Type = (session[[sessionNumber]]$feedback_type[1:length(session[[sessionNumber]]$contrast_left)] + 1) / 2
  )
  colnames(sessionTable) <- c("Session Number", "Mouse Name", "Trial Number", "Left Contrast", "Right Contrast", "Average Spikes", "Feedback Type")
  return(sessionTable)
}
```

Using the best and the worst success rate sessions may introduce bias, so let's take half from each session and combine them together. We can do so by randomizing each session 1 and session 17, taking half from each, then combining them into one table.

```{r cache=TRUE}
set.seed(49)
session1 <- tableFunction(1)
randomSession1 <- session1[sample(nrow(session1)), ]
set.seed(49)
session17 <- tableFunction(17)
randomSession17 <- session17[sample(nrow(session17)), ]
session1.17 <- rbind(randomSession1[1:(nrow(session1)/2),], 
                     randomSession17[1:(nrow(session17)/2),])
set.seed(49)
session1.17 <- session1.17[sample(nrow(session1.17)), ]
kable(head(session1.17), format = "html", table.attr = "class='table table-striped'", digits = 2)
```

Let's do something similar with all the sessions. We will take 10 random trials from each session 1 to 18 and make a table.

```{r cache = TRUE}
set.seed(49)
whole_df <- data.frame()
for (i in 1:18){
  set.seed(49)
  temp_dataFrame <- tableFunction(i)
  temp_dataFrame <- temp_dataFrame[sample(nrow(temp_dataFrame)), ]
  whole_df <- rbind(whole_df, temp_dataFrame[1:10,])
}
set.seed(49)
whole_df <- whole_df[sample(nrow(whole_df)), ]
kable(head(whole_df), format = "html", table.attr = "class='table table-striped'", digits = 2)
```

## Section 4 Predictive modeling

To make my predictive model, I will use each of the tables above and use logistic regression, linear discriminant analysis (LDA) and k nearest neighbors (kNN). Once I make these models, I will compare the accuracy rates and misclassification rates of each model to pick the model with a high accuracy rate, but low misclassification rate.

```{r cache=TRUE}
logModel <- function(train, test) {
  log.model <- glm(`Feedback Type` ~ `Left Contrast` + `Right Contrast` + `Average Spikes`,
                      data = train, 
                      family = "binomial")
  log.prediction <- predict(log.model, newdata = test, type = "response")
  log.prediction.label <- ifelse(log.prediction > 0.5, 1, 0)
  accuracy <- mean(log.prediction.label == test$`Feedback Type`)
  matrix <- table(test$`Feedback Type`, log.prediction.label)
  miscalculation <- 1 - sum(diag(matrix)) / sum(matrix)
  ROC <- roc(test$`Feedback Type`, log.prediction)
  AUC <- auc(ROC)
  return(list(accuracy = accuracy, miscalculation = miscalculation, AUC = AUC))
}
ldaModel <- function(train, test){
  lda.model <- lda(`Feedback Type` ~ `Left Contrast` + `Right Contrast` + 
                     `Average Spikes`,
                   data = train)
  lda.prediction <- predict(lda.model, newdata = test)
  lda.matrix <- table(lda.prediction$class, test$`Feedback Type`)
  lda.misclassifcation.rate <- 1 - sum(diag(lda.matrix)) / sum(lda.matrix)
  lda.accuracy  <- mean(lda.prediction$class == test$`Feedback Type`)
  ROC <- roc(test$`Feedback Type`, lda.prediction$posterior[, 2])
  AUC <- auc(ROC)
  return(list(misclassification = lda.misclassifcation.rate, accuracy = lda.accuracy,AUC = AUC))
}

kNearest <- function(train, test, kn){
  KNN <- knn(as.matrix(train$`Average Spikes`), as.matrix(test$`Average Spikes`), cl = 
               train$`Feedback Type`, k = kn)
  matrix <- table(test$`Feedback Type`, KNN)
  error <- 1 - sum(diag(matrix)) / sum(matrix)
  accuracy <- mean(KNN == test$`Feedback Type`)
  return(list(KNN = KNN, error = error, accuracy = accuracy))
}
trainTestData <- function(dataFrame) {
  set.seed(49)
  len <- nrow(dataFrame)
  test <- dataFrame[sample(len), ]
  first20 <- as.integer(len*.2)
  testData <- test[1:first20,]
  testData <- na.omit(testData)
  trainData <- test[first20 + 1:len,]
  trainData <- na.omit(trainData)
  return(list(testData = testData, trainData = trainData))
}
```

### Base Model

We will use this table to make our model.

```{r cache = TRUE}
kable(head(data_integration), format = "html", table.attr = "class='table table-striped'",digits=2) 
```

To guide this model, I will use 80% of the data to train my predictive model and check how accurate and missclassification rate with the rest of the 20%

```{r cache = TRUE}
suppressWarnings({
baseTest <- data_integration[1:4,]
baseTrain <- data_integration[5:18,]


base.log.model <- glm(`Success Rate` ~ `Left Contrast` + `Right Contrast` + `Average Spikes`,
                      data = baseTrain, 
                      family = "gaussian")
base.log.prediction <- predict(base.log.model, newdata = baseTest, type = "response")

base.prediction.label <- ifelse(base.log.prediction > 0.5, 1, 0)

base.log.accuracy <- mean(base.prediction.label == baseTest$`Success Rate`)
base.log.matrix <- table(baseTest$`Success Rate`, base.prediction.label)
base.mis.cal <- 1 - sum(diag(base.log.matrix)) / sum(base.log.matrix)
base.roc <- roc(baseTest$`Success Rate`, base.log.prediction)
base.auc <- auc(base.roc)

Base.model <- tibble(
  base.Accuracy = base.log.accuracy,
  base.Mis = base.mis.cal,
  base.AUC = base.auc[1]
)
colnames(Base.model) <- c("Accuracy","Misclassification Rate","Area Under Curve")
kable(Base.model, format = "html", table.attr = "class='table table-striped'",digits=2)
})
```

The table above uses Logistic Regression and lists the accuracy, misclassification rate, and area under the curve. Since the accuracy rate is low, and the misclassification rate is high, this is not a good model. Since the rates are so bad, the problem may be caused by the table itself. It would not be useful to keep using my base table.

Let's try looking at the second table I made.

### Session 1 and 17 Model

```{r cache = TRUE}
kable(head(session1.17), format = "html", table.attr = "class='table table-striped'", digits = 2)
```

This table may be better since it takes in more sessions than the previous base model. I will use 80% of the data for training and 20% for testing.

```{r cache = TRUE}
suppressWarnings({
session1.17.df <- data.frame()

session1.17testTrain <- trainTestData(session1.17)
session1.17test <- session1.17testTrain$testData
session1.17train <- session1.17testTrain$trainData

log.session1.17 <-logModel(session1.17train, session1.17test)
session1.17.df[1,1] <- log.session1.17$accuracy
session1.17.df[2,1] <- log.session1.17$miscalculation

lda.session1.17 <- ldaModel(session1.17train, session1.17test)
session1.17.df[1,2] <- lda.session1.17$accuracy
session1.17.df[2,2] <- lda.session1.17$misclassification

k1.lda.session1.17 <- kNearest(session1.17train, session1.17test, 1)
session1.17.df[1,3] <- k1.lda.session1.17$accuracy
session1.17.df[2,3] <- k1.lda.session1.17$error

k5.lda.session1.17 <- kNearest(session1.17train, session1.17test, 5)
session1.17.df[1,4] <- k1.lda.session1.17$accuracy
session1.17.df[2,4] <- k1.lda.session1.17$error

colnames(session1.17.df) <- c("Logistic Regression", "LDA", "KNN with K = 1", "KNN with K = 5")
rownames(session1.17.df) <- c("Accuracy","Misclassification Rate")
kable(session1.17.df, format = "html", table.attr = "class='table table-striped'",digits=2)
})
```

Firstly, we see that the accuracy and the misclassification rate for logistic regression is equal. This indicates that this model preforms no better than random guessing.

With KNN, the choice of k does not matter. Although it has a high accuracy rate and low miclassification rate, we should look for the last model.

Out of the 4 models I tested, LDA was the best since it has the highest accuracy rate and the lowest misclassifcation rate.

### Whole Model

```{r cache=TRUE}
kable(head(whole_df), format = "html", table.attr = "class='table table-striped'", digits = 2)
```

Using more data could make the model perform better. Thus, I am choosing to use the combined table of all the sessions. 80% of the data will be used for training while the left over 20% would be used for testing.

```{r cache=TRUE}
whole.df <- data.frame()

wholeTestTrain <- trainTestData(whole_df)
wholeTest <- (wholeTestTrain$testData)
wholeTrain <- (wholeTestTrain$trainData)

log.whole <-logModel(wholeTrain, wholeTest)
whole.df[1,1] <- log.whole$accuracy
whole.df[2,1] <- log.whole$miscalculation

lda.whole <- ldaModel(wholeTrain, wholeTest)
whole.df[1,2] <- lda.whole$accuracy
whole.df[2,2] <- lda.whole$misclassification

k1.whole <- kNearest(wholeTrain, wholeTest, 1)
whole.df[1,3] <- k1.whole$accuracy
whole.df[2,3] <- k1.whole$error

k5.whole <- kNearest(wholeTrain, wholeTest, 5)
whole.df[1,4] <- k5.whole$accuracy
whole.df[2,4] <- k5.whole$error

colnames(whole.df) <- c("Logistic Regression", "LDA", "KNN with K = 1", "KNN with K = 5")
rownames(whole.df) <- c("Accuracy","Misclassification Rate")

kable(whole.df, format = "html", table.attr = "class='table table-striped'",digits=2)
```

We can see that the accuracy and the misclassification rate for logistic regression is equal to the LDA. This indicates that these two models are equally as effective as each other. The accuracy of both is decently high. The misclassifcation rate is also decenetly low. These two models show the best performance out of the four.

With KNN, k = 5 is higher than k = 1. The k = 5 case shows a decently high accuracy rate and low misclassification rate. However, LDA and logistic regression show a better model.

With the test set, I will use the LDA model from my mix of session 1 and session 2. I will use this as my predictive model since it has the highest accuracy rating and lowest misclassification rate.

## Section 5 Prediction performance on the test sets

## Section 6 Discussion

To start with this research, I graphed many variables against the success rate. I saw patterns relating the success rate to average neuron spikes, left contrast, right contrast and names. However, I decided to only integrate neuron spikes, left contrast and right contrast. I made this decision because I plan to test my model against a wide range of mice and I did not want my data to be biased towards specific mice. The types of data I decided to integrate were averages of all 18 sessions, a combination of the best and worst sessions, and 10 random trials from all 18 sessions.

For all my models, I took 80% of the data for the purpose of training, and used 20% of the data for the purpose of testing. I decided to do the average of all 18 sessions because I wanted a base line of how the predictive model would look. This was not a good idea since my predictive models were not accurate. This may be because my model did not have enough training data to make a proper prediction. I then decided to make a model comprised of the worst session and the best session. If my predictive model could predict the worse session and the best session, then it could predict anything in between. Lastly, I decided to use a model with 10 trials from all 18 sessions because I wanted a more varied data set.

As for the type of models, I used logistic regression, linear discriminant analysis and k nearest neighbors. I found that my mixed model of the worst and best session gave me the best results, especially using the LDA model.
